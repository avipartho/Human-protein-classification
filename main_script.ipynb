{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import time \n",
    "import json \n",
    "import torch \n",
    "import random \n",
    "import warnings\n",
    "import torchvision\n",
    "import numpy as np \n",
    "import pandas as pd \n",
    "import sys \n",
    "import shutil\n",
    "import pathlib\n",
    "\n",
    "from tqdm import tqdm \n",
    "from datetime import datetime\n",
    "from torch import nn,optim\n",
    "from collections import OrderedDict\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import Dataset,DataLoader\n",
    "from torch.optim import lr_scheduler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from timeit import default_timer as timer\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from iterstrat.ml_stratifiers import MultilabelStratifiedKFold\n",
    "from torchvision import models\n",
    "from pretrainedmodels.models import bninception\n",
    "from collections import OrderedDict\n",
    "import torch.nn.functional as F\n",
    "from torchsummary import summary\n",
    "from PIL import Image\n",
    "from torchvision import transforms as T\n",
    "from imgaug import augmenters as iaa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. set random seed\n",
    "random.seed(2050)\n",
    "np.random.seed(2050)\n",
    "torch.manual_seed(2050)\n",
    "torch.cuda.manual_seed_all(2050)\n",
    "torch.backends.cudnn.benchmark = True\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(\"./logs/\"):\n",
    "    os.mkdir(\"./logs/\")\n",
    "\n",
    "def setup_logger(fold):\n",
    "    log = Logger()\n",
    "    log.open(\"logs/%s_fold_%s_log_train.txt\"%(config.model_name,str(fold)),mode=\"a\")\n",
    "    log.write(\"\\n----------------------------------------------- [START %s] %s\\n\\n\" % (datetime.now().strftime('%Y-%m-%d %H:%M:%S'), '-' * 51))\n",
    "    log.write('                           |------------ Train -------------|----------- Valid -------------|----------Best Results---------|------------|\\n')\n",
    "    log.write('mode     iter     epoch    |         loss   f1_macro        |         loss   f1_macro       |         loss   f1_macro       | time       |\\n')\n",
    "    log.write('-------------------------------------------------------------------------------------------------------------------------------\\n')\n",
    "    return log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DefaultConfigs(object):\n",
    "    # DIRECTORIES\n",
    "    train_data = \"../input/human-protein-atlas-image-classification/data/train/\" # train data directory\n",
    "    test_data = \"../input/human-protein-atlas-image-classification/data/test/\"   # test data directory\n",
    "    data_root = \"../input/human-protein-atlas-image-classification/data/\"        # data root directory\n",
    "    weights = \"./checkpoints/\"                                                   # saved models' directory\n",
    "    best_models = \"./checkpoints/best_models/\"                                   # best models' directory\n",
    "    submit = \"./submit/\"                                                         # submission file directory\n",
    "\n",
    "    model_name = \"bninception_bcelog4\"\n",
    "\n",
    "    # PARAMETERS\n",
    "    num_classes = 28\n",
    "    img_width = 512\n",
    "    img_height = 512\n",
    "    channels = 4\n",
    "    lr = 0.03\n",
    "    batch_size = 32\n",
    "    epochs = 50\n",
    "    curr_fold = 0 \n",
    "\n",
    "    # FLAGS\n",
    "    first_layer_pretrained = True\n",
    "    oversample = True\n",
    "    \n",
    "    train = True\n",
    "    retrain = True # train flag should be True for retrain to work\n",
    "    test = True\n",
    "    ensemble = False\n",
    "\n",
    "config = DefaultConfigs()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HumanDataset(Dataset):\n",
    "    def __init__(self,images_df,base_path,augument=True,mode=\"train\"):\n",
    "        if not isinstance(base_path, pathlib.Path):\n",
    "            base_path = pathlib.Path(base_path)\n",
    "        self.images_df = images_df.copy()\n",
    "        self.augument = augument\n",
    "        self.images_df.Id = self.images_df.Id.apply(lambda x:base_path / x)\n",
    "        self.mlb = MultiLabelBinarizer(classes = np.arange(0,config.num_classes))\n",
    "        self.mlb.fit(np.arange(0,config.num_classes))\n",
    "        self.mode = mode\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images_df)\n",
    "\n",
    "    def __getitem__(self,index):\n",
    "        X = self.read_images(index)\n",
    "        if not self.mode == \"test\":\n",
    "            labels = np.array(list(map(int, self.images_df.iloc[index].Target.split(' '))))\n",
    "            y  = np.eye(config.num_classes,dtype=np.float)[labels].sum(axis=0)\n",
    "        else:\n",
    "            y = str(self.images_df.iloc[index].Id.absolute())\n",
    "        \n",
    "        if self.augument:\n",
    "            X = self.augumentor(X)\n",
    "        \n",
    "        # X = T.Compose([T.ToPILImage(),T.ToTensor(),T.Normalize([0.08069, 0.05258, 0.05487, 0.08282], [0.13704, 0.10145, 0.15313, 0.13814])])(X)\n",
    "        X = T.Compose([T.ToPILImage(),T.ToTensor()])(X)\n",
    "        \n",
    "        return X.float(),y\n",
    "\n",
    "    def read_images(self,index):\n",
    "        row = self.images_df.iloc[index]\n",
    "        filename = str(row.Id.absolute())\n",
    "        #use only rgb channels\n",
    "        if config.channels == 4:\n",
    "            images = np.zeros(shape=(512,512,4))\n",
    "        else:\n",
    "            images = np.zeros(shape=(512,512,3))\n",
    "        r = np.array(Image.open(filename+\"_red.png\")) \n",
    "        g = np.array(Image.open(filename+\"_green.png\")) \n",
    "        b = np.array(Image.open(filename+\"_blue.png\")) \n",
    "        y = np.array(Image.open(filename+\"_yellow.png\")) \n",
    "        images[:,:,0] = r.astype(np.uint8) \n",
    "        images[:,:,1] = g.astype(np.uint8)\n",
    "        images[:,:,2] = b.astype(np.uint8)\n",
    "        if config.channels == 4:\n",
    "            images[:,:,3] = y.astype(np.uint8)\n",
    "        images = images.astype(np.uint8)\n",
    "        #images = np.stack(images,-1) \n",
    "        if config.img_height == 512:\n",
    "            return images\n",
    "        else:\n",
    "            return cv2.resize(images,(config.img_width,config.img_height))\n",
    "\n",
    "    def augumentor(self,image):\n",
    "        augment_img = iaa.Sequential([\n",
    "            iaa.OneOf([\n",
    "                iaa.Affine(rotate=90),\n",
    "                iaa.Affine(rotate=180),\n",
    "                iaa.Affine(rotate=270),\n",
    "                iaa.Affine(shear=(-16, 16)),\n",
    "                iaa.Fliplr(0.5),\n",
    "                iaa.Flipud(0.5),\n",
    "            ])], random_order=True)\n",
    "        \n",
    "        image_aug = augment_img.augment_image(image)\n",
    "        return image_aug"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save best model\n",
    "def save_checkpoint(state, is_best_loss,is_best_f1,fold):\n",
    "    filename = config.weights + config.model_name + os.sep +str(fold) + os.sep + \"checkpoint.pth.tar\"\n",
    "    torch.save(state, filename)\n",
    "    if is_best_loss:\n",
    "        shutil.copyfile(filename,\"%s/%s_fold_%s_model_best_loss.pth.tar\"%(config.best_models,config.model_name,str(fold)))\n",
    "    if is_best_f1:\n",
    "        shutil.copyfile(filename,\"%s/%s_fold_%s_model_best_f1.pth.tar\"%(config.best_models,config.model_name,str(fold)))\n",
    "\n",
    "# evaluate meters\n",
    "class AverageMeter(object):\n",
    "    \"\"\"Computes and stores the average and current value\"\"\"\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count\n",
    "\n",
    "# print logger\n",
    "class Logger(object):\n",
    "    def __init__(self):\n",
    "        self.terminal = sys.stdout  #stdout\n",
    "        self.file = None\n",
    "\n",
    "    def open(self, file, mode=None):\n",
    "        if mode is None: mode ='w'\n",
    "        self.file = open(file, mode)\n",
    "\n",
    "    def write(self, message, is_terminal=1, is_file=1 ):\n",
    "        if '\\r' in message: is_file=0\n",
    "\n",
    "        if is_terminal == 1:\n",
    "            self.terminal.write(message)\n",
    "            self.terminal.flush()\n",
    "            #time.sleep(1)\n",
    "\n",
    "        if is_file == 1:\n",
    "            self.file.write(message)\n",
    "            self.file.flush()\n",
    "\n",
    "    def flush(self):\n",
    "        # this flush method is needed for python 3 compatibility.\n",
    "        # this handles the flush command by doing nothing.\n",
    "        # you might want to specify some extra behavior here.\n",
    "        pass\n",
    "\n",
    "class Oversampling:\n",
    "    def __init__(self, df):\n",
    "        self.train_labels = df.set_index('Id')\n",
    "        self.train_labels['Target'] = [[int(i) for i in s.split()] for s in self.train_labels['Target']]\n",
    "        # set the minimum number of duplicates for each class\n",
    "        self.multi = [1, 1, 1, 1, 1, 1, 1, 1, 8, 8,\n",
    "                      8, 1, 1, 1, 1, 8, 1, 2, 1, 1,\n",
    "                      4, 1, 1, 1, 2, 1, 2, 8]\n",
    "        # TODO : different oversampling? https://www.kaggle.com/wordroid/inceptionresnetv2-resize256-f1loss-lb0-419\n",
    "\n",
    "    def get(self, image_id):\n",
    "        labels = self.train_labels.loc[image_id, 'Target'] if image_id in self.train_labels.index else []\n",
    "        m = 1\n",
    "        for l in labels:\n",
    "            if m < self.multi[l]: m = self.multi[l]\n",
    "        return m\n",
    "        \n",
    "def get_learning_rate(optimizer):\n",
    "    lr=[]\n",
    "    for param_group in optimizer.param_groups:\n",
    "       lr +=[ param_group['lr'] ]\n",
    "\n",
    "    #assert(len(lr)==1) #we support only one param_group\n",
    "    lr = lr[0]\n",
    "\n",
    "    return lr\n",
    "\n",
    "def time_to_str(t, mode='min'):\n",
    "    if mode=='min':\n",
    "        t  = int(t)/60\n",
    "        hr = t//60\n",
    "        min = t%60\n",
    "        return '%2d hr %02d min'%(hr,min)\n",
    "\n",
    "    elif mode=='sec':\n",
    "        t   = int(t)\n",
    "        min = t//60\n",
    "        sec = t%60\n",
    "        return '%2d min %02d sec'%(min,sec)\n",
    "\n",
    "    else:\n",
    "        raise NotImplementedError\n",
    "\n",
    "def get_best_thres(val_loader,model,num_split=100):\n",
    "\n",
    "    y_true_all, y_pred_all = [], []\n",
    "    model.cuda()\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for (images,target) in tqdm(val_loader):\n",
    "            images_var = images.cuda(non_blocking=True)\n",
    "            target = np.array(target)\n",
    "            output = model(images_var).sigmoid().cpu().data.numpy()\n",
    "            y_true_all.extend(target)\n",
    "            y_pred_all.extend(output)\n",
    "\n",
    "    cate2th = {}\n",
    "    y_true_all, y_pred_all = np.array(y_true_all), np.array(y_pred_all)\n",
    "\n",
    "    for c in range(28):\n",
    "        y_true = y_true_all[:,c]\n",
    "        y_pred = y_pred_all[:,c]\n",
    "        best_th = 0\n",
    "        best_f1 = -1\n",
    "\n",
    "        for th in np.linspace(0,1,num_split,endpoint=False):\n",
    "            f1 = f1_score(y_true,(y_pred > th).astype(int))\n",
    "            if best_f1 <= f1:\n",
    "                best_f1 = f1\n",
    "                best_th = th\n",
    "\n",
    "        cate2th[c] = best_th\n",
    "\n",
    "    return cate2th"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_net(show_summary=0):\n",
    "    \n",
    "    '''show_summary flag is for printing summary of the model\n",
    "        0 --> prints nothing\n",
    "        1 --> keras type model summary using torchsummary library\n",
    "        2 --> prints the model in pyTorch fashion'''\n",
    "    \n",
    "    model = bninception(pretrained=\"imagenet\")\n",
    "    model.global_pool = nn.AdaptiveAvgPool2d(1)\n",
    "    old_weight = model.conv1_7x7_s2.weight\n",
    "    model.conv1_7x7_s2 = nn.Conv2d(config.channels, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3))\n",
    "    \n",
    "    if config.first_layer_pretrained:\n",
    "        # using pretrained imagenet weight for RGB channels & G channel weight for Y channel\n",
    "        if config.channels == 3:\n",
    "            model.conv1_7x7_s2.weight = old_weight\n",
    "        else:\n",
    "            new_weight = torch.nn.Parameter(torch.cat((old_weight,torch.reshape(old_weight[:,1,:,:],(64,1,7,7))),dim=1))\n",
    "            model.conv1_7x7_s2.weight = new_weight \n",
    "\n",
    "    model.last_linear = nn.Sequential(\n",
    "                nn.BatchNorm1d(1024),\n",
    "                nn.Dropout(0.5),\n",
    "                nn.Linear(1024, config.num_classes),\n",
    "            )\n",
    "\n",
    "    if show_summary == 1: print(summary(model.to(\"cuda:0\"), (config.channels, config.img_width, config.img_height)))\n",
    "    elif show_summary == 2: print(model)\n",
    "    # to check weight\n",
    "    # for param in model.parameters():    \n",
    "    #     print(param.data[0])\n",
    "    #     break\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train, Validate, Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(train_loader,model,criterion,optimizer,epoch,valid_loss,best_results,start,log):\n",
    "    losses = AverageMeter()\n",
    "    f1 = AverageMeter()\n",
    "    model.train()\n",
    "    for i,(images,target) in enumerate(train_loader):\n",
    "        images = images.cuda(non_blocking=True)\n",
    "        target = torch.from_numpy(np.array(target)).float().cuda(non_blocking=True)\n",
    "        # compute output\n",
    "        output = model(images)\n",
    "        loss = criterion(output,target)\n",
    "        losses.update(loss.item(),images.size(0))\n",
    "        \n",
    "        f1_batch = f1_score(target,output.sigmoid().cpu() > 0.15,average='macro')\n",
    "        f1.update(f1_batch,images.size(0))\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        print('\\r',end='',flush=True)\n",
    "        message = '%s %5.1f %6.1f         |         %0.3f  %0.3f           |         %0.3f  %0.4f         |         %s  %s    | %s' % (\\\n",
    "                \"train\", i/len(train_loader), epoch,\n",
    "                losses.avg, f1.avg, \n",
    "                valid_loss[0], valid_loss[1], \n",
    "                str(best_results[0])[:8],str(best_results[1])[:8],\n",
    "                time_to_str((timer() - start),'min'))\n",
    "        print(message , end='',flush=True)\n",
    "    log.write(\"\\n\")\n",
    "    return [losses.avg,f1.avg]\n",
    "\n",
    "def evaluate(val_loader,model,criterion,epoch,train_loss,best_results,start,log):\n",
    "    # only meter loss and f1 score\n",
    "    losses = AverageMeter()\n",
    "    f1 = AverageMeter()\n",
    "    model.cuda()\n",
    "    model.eval() # switch mode for evaluation\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for i, (images,target) in enumerate(val_loader):\n",
    "            images_var = images.cuda(non_blocking=True)\n",
    "            target = torch.from_numpy(np.array(target)).float().cuda(non_blocking=True)\n",
    "            output = model(images_var)\n",
    "            loss = criterion(output,target)\n",
    "            losses.update(loss.item(),images_var.size(0))\n",
    "            f1_batch = f1_score(target,output.sigmoid().cpu().data.numpy() > 0.15,average='macro')\n",
    "            f1.update(f1_batch,images_var.size(0))\n",
    "            print('\\r',end='',flush=True)\n",
    "            message = '%s   %5.1f %6.1f         |         %0.3f  %0.3f           |         %0.3f  %0.4f         |         %s  %s    | %s' % (\\\n",
    "                    \"val\", i/len(val_loader), epoch,                    \n",
    "                    train_loss[0], train_loss[1], \n",
    "                    losses.avg, f1.avg,\n",
    "                    str(best_results[0])[:8],str(best_results[1])[:8],\n",
    "                    time_to_str((timer() - start),'min'))\n",
    "\n",
    "            print(message, end='',flush=True)\n",
    "        log.write(\"\\n\")\n",
    "    return [losses.avg,f1.avg]\n",
    "\n",
    "def test(test_loader,model,fold,test_files,metric,log,th_dic=None):\n",
    "    sample_submission_df = test_files\n",
    "    filenames,labels,submissions= [],[],[]\n",
    "    model.cuda() # confirm the model converted to cuda\n",
    "    model.eval()\n",
    "    submit_results = []\n",
    "    \n",
    "    for i,(input,filepath) in enumerate(tqdm(test_loader)):\n",
    "        # change everything to cuda and get only basename\n",
    "        filepath = [os.path.basename(x) for x in filepath]\n",
    "        with torch.no_grad():\n",
    "            image_var = input.cuda(non_blocking=True)\n",
    "            y_pred = model(image_var)\n",
    "            label = y_pred.sigmoid().cpu().data.numpy()\n",
    "            if th_dic is not None: labels.append(label > np.array(list(th_dic.values())))\n",
    "            else: labels.append(label > 0.15)\n",
    "            filenames.append(filepath)\n",
    "\n",
    "    for row in np.concatenate(labels):\n",
    "        subrow = ' '.join(list([str(i) for i in np.nonzero(row)[0]]))\n",
    "        submissions.append(subrow)\n",
    "    sample_submission_df['Predicted'] = submissions\n",
    "    sample_submission_df.to_csv('./submit/%s_%s_fold_%s_submission.csv'%(config.model_name,metric,str(fold)), index=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(fold, oversample=False):\n",
    "    log = setup_logger(fold)\n",
    "\n",
    "    # mkdirs\n",
    "    if not os.path.exists(config.submit):\n",
    "           os.makedirs(config.submit)\n",
    "    if not os.path.exists(config.weights + config.model_name + os.sep +str(fold)):\n",
    "           os.makedirs(config.weights + config.model_name + os.sep +str(fold))\n",
    "    if not os.path.exists(config.best_models):\n",
    "           os.mkdir(config.best_models)\n",
    "    if not os.path.exists(\"./logs/\"):\n",
    "           os.mkdir(\"./logs/\")\n",
    "\n",
    "    # get model\n",
    "    model = get_net()\n",
    "    model.cuda()\n",
    "\n",
    "    # criterion\n",
    "    optimizer = optim.SGD(model.parameters(),lr = config.lr,momentum=0.9,weight_decay=1e-4)\n",
    "    criterion = nn.BCEWithLogitsLoss().cuda()\n",
    "    \n",
    "    start_epoch = 0\n",
    "    best_loss = 999\n",
    "    best_f1 = 0\n",
    "    best_results = [np.inf,0]\n",
    "    val_metrics = [np.inf,0]\n",
    "    resume = False\n",
    "    all_files = pd.read_csv(config.data_root+\"train.csv\")\n",
    "    test_files = pd.read_csv(config.data_root+\"sample_submission.csv\")\n",
    "    # train_data_list,val_data_list = train_test_split(all_files,test_size = 0.13,random_state = 2050)\n",
    "\n",
    "    # Stratify \n",
    "    mlb = MultiLabelBinarizer()\n",
    "    labels = [[int(i) for i in i.split()] for i in all_files.Target.tolist()]\n",
    "    labels = mlb.fit_transform(labels)\n",
    "\n",
    "    X, Y = np.arange(len(labels)), labels\n",
    "    mskf = MultilabelStratifiedKFold(n_splits=5, random_state=0)\n",
    "\n",
    "    for n_fold, (train_index, test_index) in enumerate(mskf.split(X, Y)):\n",
    "        print('Fold %s'%str(n_fold))\n",
    "        train_data_list = all_files.iloc[train_index]\n",
    "        val_data_list = all_files.iloc[test_index]\n",
    "        if n_fold==fold: break\n",
    "    \n",
    "    # Oversample\n",
    "    if oversample:\n",
    "        oversampled_train_data = train_data_list.copy()\n",
    "        s = Oversampling(oversampled_train_data)\n",
    "        for ind,idx in enumerate(train_data_list.Id):\n",
    "            multiplier = s.get(idx)\n",
    "            if multiplier>1: \n",
    "                oversampled_train_data = oversampled_train_data.append([train_data_list.iloc[[ind]]]*(multiplier-1),ignore_index=True)\n",
    "            train_data_list = oversampled_train_data\n",
    "\n",
    "    # load dataset\n",
    "    train_gen = HumanDataset(train_data_list,config.train_data,mode=\"train\")\n",
    "    train_loader = DataLoader(train_gen,batch_size=config.batch_size,shuffle=True,pin_memory=True,num_workers=4)\n",
    "\n",
    "    val_gen = HumanDataset(val_data_list,config.train_data,augument=False,mode=\"train\")\n",
    "    val_loader = DataLoader(val_gen,batch_size=config.batch_size,shuffle=False,pin_memory=True,num_workers=4)\n",
    "\n",
    "    test_gen = HumanDataset(test_files,config.test_data,augument=False,mode=\"test\")\n",
    "    test_loader = DataLoader(test_gen,1,shuffle=False,pin_memory=True,num_workers=4)\n",
    "       \n",
    "    scheduler = lr_scheduler.StepLR(optimizer,step_size=10,gamma=0.1)\n",
    "    start = timer()\n",
    "\n",
    "    if config.retrain: # retrain\n",
    "        saved_model = torch.load(\"%s/%s/%s/checkpoint.pth.tar\"%(config.weights,config.model_name,str(fold)))\n",
    "        model.load_state_dict(saved_model[\"state_dict\"])\n",
    "        optimizer.load_state_dict(saved_model[\"optimizer\"])\n",
    "        start_epoch = saved_model[\"epoch\"]\n",
    "        best_results = [saved_model[\"best_loss\"],saved_model[\"best_f1\"]]\n",
    "        scheduler = lr_scheduler.StepLR(optimizer,step_size=10,gamma=0.1)\n",
    "        del(saved_model)\n",
    "\n",
    "    if config.train: # train\n",
    "        for epoch in range(start_epoch,config.epochs):\n",
    "            scheduler.step(epoch)\n",
    "            # train\n",
    "            lr = get_learning_rate(optimizer)\n",
    "            train_metrics = train(train_loader,model,criterion,optimizer,epoch,val_metrics,best_results,start,log)\n",
    "            # validate\n",
    "            val_metrics = evaluate(val_loader,model,criterion,epoch,train_metrics,best_results,start,log)\n",
    "            # check results \n",
    "            is_best_loss = val_metrics[0] < best_results[0]\n",
    "            best_results[0] = min(val_metrics[0],best_results[0])\n",
    "            is_best_f1 = val_metrics[1] > best_results[1]\n",
    "            best_results[1] = max(val_metrics[1],best_results[1])   \n",
    "            # save model\n",
    "            save_checkpoint({\n",
    "                        \"epoch\":epoch + 1,\n",
    "                        \"model_name\":config.model_name,\n",
    "                        \"state_dict\":model.state_dict(),\n",
    "                        \"best_loss\":best_results[0],\n",
    "                        \"optimizer\":optimizer.state_dict(),\n",
    "                        \"fold\":fold,\n",
    "                        \"best_f1\":best_results[1],\n",
    "            },is_best_loss,is_best_f1,fold)\n",
    "            # print logs\n",
    "            print('\\r',end='',flush=True)\n",
    "            log.write('%s  %5.1f %6.1f         |         %0.3f  %0.3f           |         %0.3f  %0.4f         |         %s  %s    | %s' % (\\\n",
    "                    \"best\", epoch, epoch,                    \n",
    "                    train_metrics[0], train_metrics[1], \n",
    "                    val_metrics[0], val_metrics[1],\n",
    "                    str(best_results[0])[:8],str(best_results[1])[:8],\n",
    "                    time_to_str((timer() - start),'min'))\n",
    "                )\n",
    "            log.write(\"\\n\")\n",
    "            time.sleep(0.01)\n",
    "\n",
    "    if config.test: # test\n",
    "        best_model = torch.load(\"%s/%s_fold_%s_model_best_loss.pth.tar\"%(config.best_models,config.model_name,str(fold)))\n",
    "        model.load_state_dict(best_model[\"state_dict\"])\n",
    "        test(test_loader,model,fold,test_files,\"best_loss\",log)\n",
    "\n",
    "        best_model = torch.load(\"%s/%s_fold_%s_model_best_f1.pth.tar\"%(config.best_models,config.model_name,str(fold)))\n",
    "        model.load_state_dict(best_model[\"state_dict\"])\n",
    "        test(test_loader,model,fold,test_files,\"best_f1\",log)\n",
    "\n",
    "        best_model = torch.load(\"%s/%s/%s/checkpoint.pth.tar\"%(config.weights,config.model_name,str(fold)))\n",
    "        model.load_state_dict(best_model[\"state_dict\"])\n",
    "        test(test_loader,model,fold,test_files,\"last_epoch\",log)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    main(config.curr_fold, config.oversample)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
